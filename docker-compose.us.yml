# Docker Compose file for OpenRouteService - US Production Setup
# Optimized for Hetzner server deployment covering the entire United States
# Documentation: https://giscience.github.io/openrouteservice/run-instance/running-with-docker

services:
  ors-app:
    build:
      context: ./
      target: publish
    container_name: ors-app-us
    restart: unless-stopped
    ports:
      - "8080:8082" # Expose the ORS API on port 8080
      - "9001:9001" # JMX monitoring port
    image: local/openrouteservice:latest
    volumes:
      - ./ors-docker:/home/ors # Mount the ORS application directory
    environment:
      # Graph building - set to True on first run or when updating OSM data
      # Set to True to rebuild graphs with new MMAP configuration
      REBUILD_GRAPHS: False # Set to True when building merged US file
      CONTAINER_LOG_LEVEL: INFO

      # Configuration file location (will be created if not exists)
      ORS_CONFIG_LOCATION: /home/ors/config/ors-config.yml

      # GC thread optimization - reduce threads to save memory
      PARALLEL_GC_THREADS: 2

      # ------------------ JAVA Memory Settings ------------------ #
      # US PBF is ~5-10GB, so for driving-car profile: 10GB * 1 profile * 2 = 20GB minimum
      # During graph building, memory needs can spike significantly higher (3-4x during peak)
      # IMPORTANT: For 32GB RAM server, XMX must be <= 24g to leave room for OS (4-6GB) and Docker overhead (2-4GB)
      # Adjust based on your Hetzner server RAM:
      # - 32GB RAM server: XMS=16g, XMX=24g (MAX - leaves 8GB for OS/Docker)
      # - 64GB RAM server: XMS=32g, XMX=56g
      # - 128GB RAM server: XMS=64g, XMX=112g
      # Setting XMS=XMX prevents heap resizing overhead and improves performance
      # Using direct US file (not merged) - try with filtered highways-only version
      # If still OOM, reduce these further
      XMS: 14g
      XMX: 24g # Max for 32GB server - leaves 8GB for OS/Docker
      # Additional JVM options for better memory management during PBF parsing
      # Aggressive GC settings to free memory faster during graph building
      # -XX:MaxGCPauseMillis=200 - Target GC pause time
      # -XX:G1HeapRegionSize=16m - Larger regions for better efficiency
      # -XX:InitiatingHeapOccupancyPercent=40 - Start GC even earlier (was 45)
      # -XX:ConcGCThreads=4 - Concurrent GC threads
      # -XX:+UseStringDeduplication - Reduce memory for duplicate strings
      # -XX:MaxDirectMemorySize=2g - Limit off-heap memory
      # -XX:ParallelGCThreads=2 - Reduce GC threads to save memory
      # -XX:ReservedCodeCacheSize=256m - Limit code cache
      # -XX:InitialCodeCacheSize=64m - Smaller initial code cache
      ADDITIONAL_JAVA_OPTS: "-XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/ors/logs/heapdump.hprof -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=16m -XX:InitiatingHeapOccupancyPercent=40 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=2 -XX:+UseStringDeduplication -XX:MaxDirectMemorySize=2g -XX:ReservedCodeCacheSize=256m -XX:InitialCodeCacheSize=64m"

      # ----------------- ORS Configuration via ENV (optional) ------------------- #
      # These can override settings in ors-config.yml
      # Uncomment and modify as needed:
      #ors.engine.profile_default.build.source_file: /home/ors/files/us-latest.osm.pbf
      #ors.engine.profile_default.graph_path: /home/ors/graphs
      #ors.engine.elevation.cache_path: /home/ors/elevation_cache
      #ors.engine.profiles.driving-car.enabled: true
      #logging.level.org.heigit: INFO

    # Docker memory limits - ensure container has enough memory
    # Set to at least XMX + 4GB for overhead (JVM overhead + OS + Docker)
    # For XMX=26g, set limit to 30g (fits within 32GB server, leaves 2GB for host)
    deploy:
      resources:
        limits:
          memory: 28g # Reduced to fit 32GB server better
        reservations:
          memory: 14g

    # Healthcheck configuration
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8082/ors/v2/health || exit 1
      start_period: 5m # Allow time for initial graph build
      interval: 30s
      timeout: 5s
      retries: 3
      disable: false
# Optional: Use Docker managed volumes instead of bind mounts
# Uncomment below and remove ./ prefix from volume paths above if preferred
#volumes:
#    graphs:
#    elevation_cache:
#    config:
#    logs:
#    files:

